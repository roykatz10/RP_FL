{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c8f135d-f5c0-4b80-82b9-163e7f0f5df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0711e382-1a20-46c8-bc55-e4e98fbb3b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fl = nn.Flatten()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(27648, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,2)\n",
    "        )\n",
    "        # self.conv_layers = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 3, 5),\n",
    "        #     # nn.ReLU(),\n",
    "        #     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        #     nn.Conv2d(3, 3, 3),\n",
    "        #     # nn.ReLU(),\n",
    "        #     nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # self.lin_layers = nn.Sequential(\n",
    "        #     # nn.Flatten(),         \n",
    "        #     nn.Linear(1452, 100),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(100,2),\n",
    "        #     nn.Softmax(dim=2)\n",
    "        # )\n",
    "\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        # x = self.fl(x)\n",
    "        return self.lin_layers(x)\n",
    "\n",
    "    def train(self, X_train, y_train, opt):\n",
    "        crit = nn.CrossEntropyLoss()\n",
    "\n",
    "        opt.zero_grad()\n",
    "        out = self.forward(X_train)\n",
    "        # print(y_train.shape)\n",
    "        loss = crit(out, y_train)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    def test(self, X_test, y_test):\n",
    "        corr, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            out = self.forward(X_test)\n",
    "            _, pred = torch.max(out.data, 1)\n",
    "            total += y_test.size(0)\n",
    "            corr += (pred == y_test).sum().item()\n",
    "        accuracy = corr / total\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6156b322-f854-448c-abae-57cb7eab6da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "X_train = torch.load(\"processed/X_train_10000.pt\")\n",
    "y_train = torch.load(\"processed/Y_train_10000.pt\").long()\n",
    "X_test = torch.load(\"processed/X_test_10000.pt\")\n",
    "y_test = torch.load(\"processed/Y_test_10000.pt\").long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a28d95-5bda-48fd-b0ba-8e3e6348debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise data to be between 0 and 1 (also changes them into floats, so * 8 in ram!)\n",
    "X_train = (X_train / 255).double()\n",
    "X_test = (X_test / 255).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01f0e61d-d996-4abb-b6b6-1aac45a06275",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:75] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 40627200000 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m n_bat \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(X_train\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m bs)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# for b in range(n_bat):\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#     X_bat = X_train[b*bs:(b+1)*bs,...]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#     y_bat = y_train[b*bs:(b+1)*bs]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m#     n.train(X_bat, y_bat, opt)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     accuracies[e] \u001b[38;5;241m=\u001b[39m n\u001b[38;5;241m.\u001b[39mtest(X_test, y_test)\n",
      "Cell \u001b[0;32mIn[2], line 40\u001b[0m, in \u001b[0;36mnet.train\u001b[0;34m(self, X_train, y_train, opt)\u001b[0m\n\u001b[1;32m     37\u001b[0m crit \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     39\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 40\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# print(y_train.shape)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m crit(out, y_train)\n",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m, in \u001b[0;36mnet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# x = self.fl(x)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/flower/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/flower/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/flower/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/flower/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/flower/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:75] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 40627200000 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "epochs = 10\n",
    "lr = 0.005\n",
    "n = net().double()\n",
    "# opt = torch.optim.Adam(n.parameters(), lr=  0.5)\n",
    "opt = torch.optim.SGD(n.parameters(), lr = lr)\n",
    "bs = 40\n",
    "\n",
    "starting_params = copy.deepcopy(n.state_dict())\n",
    "\n",
    "accuracies = np.zeros(epochs)\n",
    "n_bat = math.ceil(X_train.size(0) / bs)\n",
    "for e in range(epochs):\n",
    "    # for b in range(n_bat):\n",
    "    #     X_bat = X_train[b*bs:(b+1)*bs,...]\n",
    "    #     y_bat = y_train[b*bs:(b+1)*bs]\n",
    "    #     n.train(X_bat, y_bat, opt)\n",
    "    n.train(X_train, y_train, opt)\n",
    "    accuracies[e] = n.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e6571ba-4147-4c46-b648-188dc81816cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv_layers.0.weight',\n",
       "              tensor([[[[ 2.8877e-02, -1.2187e-03,  3.7166e-02, -1.5397e-02,  2.0078e-02],\n",
       "                        [ 1.1238e-01,  1.4904e-02, -5.6236e-02,  5.5585e-02,  3.4092e-02],\n",
       "                        [ 1.0816e-01, -1.6880e-02,  7.3981e-02,  2.3245e-02,  4.5177e-02],\n",
       "                        [-5.5714e-02, -2.8775e-04,  3.1509e-03,  1.1459e-01, -2.5760e-03],\n",
       "                        [ 5.0603e-02,  5.9113e-02, -6.3772e-02, -5.1460e-02,  8.3780e-02]],\n",
       "              \n",
       "                       [[-9.0593e-02,  8.7437e-02, -7.6303e-02,  1.1363e-01, -7.3244e-02],\n",
       "                        [-1.0310e-01, -3.2267e-02,  9.2827e-02, -2.1913e-02,  8.2073e-02],\n",
       "                        [ 3.1165e-02,  6.6347e-02, -9.2051e-02,  6.0310e-02, -9.3667e-02],\n",
       "                        [-8.5700e-02, -1.6100e-02,  1.3944e-03, -9.9361e-02, -1.0418e-01],\n",
       "                        [ 3.2710e-02,  2.4436e-02, -9.4154e-03, -2.2422e-02,  5.9513e-04]],\n",
       "              \n",
       "                       [[ 5.2131e-02, -1.0073e-01,  1.0143e-01,  4.5356e-02, -8.2588e-02],\n",
       "                        [ 1.0105e-01,  7.7169e-02,  6.1904e-02, -6.6539e-02, -2.8273e-03],\n",
       "                        [ 4.2672e-05, -1.1300e-01, -4.3277e-02, -3.8400e-02,  7.9440e-03],\n",
       "                        [-1.0435e-01,  5.6764e-02, -6.8360e-02,  9.8708e-02,  3.9213e-02],\n",
       "                        [ 4.3001e-03, -5.0907e-02,  2.7993e-02,  5.7173e-02,  5.4055e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.5451e-02, -7.3714e-02, -9.7891e-02,  6.3351e-02, -6.0491e-02],\n",
       "                        [ 1.0419e-01, -4.9117e-02, -2.8966e-02,  6.6371e-02, -4.1108e-02],\n",
       "                        [-2.6608e-02, -6.8851e-02,  6.1535e-02,  5.2291e-02, -5.9745e-02],\n",
       "                        [-4.4028e-02, -1.0871e-01,  7.8717e-02, -9.5424e-02,  1.0438e-01],\n",
       "                        [ 5.9068e-02, -9.6212e-02,  6.7577e-02,  6.7550e-02,  1.1302e-01]],\n",
       "              \n",
       "                       [[-1.5818e-02, -3.5957e-02, -8.3069e-02,  1.4469e-02,  3.6513e-02],\n",
       "                        [ 7.7590e-02,  1.1176e-01,  2.3498e-04,  1.0240e-01,  2.3491e-02],\n",
       "                        [ 4.7222e-02,  1.0383e-01,  3.4933e-02,  2.9772e-02, -8.9418e-03],\n",
       "                        [-1.0758e-02,  7.9643e-02,  5.8936e-02,  3.5481e-02, -2.6247e-02],\n",
       "                        [ 1.1238e-01,  5.2436e-02,  7.2259e-02,  4.3377e-02,  9.1212e-02]],\n",
       "              \n",
       "                       [[ 4.0666e-02, -4.3004e-02,  5.8436e-02, -7.9726e-02,  2.2033e-02],\n",
       "                        [ 2.3848e-02, -7.3618e-02,  3.5081e-02,  7.2937e-02, -3.3705e-02],\n",
       "                        [ 9.1368e-02, -3.1772e-02, -7.7307e-02,  9.4827e-03,  5.7782e-02],\n",
       "                        [ 9.6373e-02,  5.4724e-02, -8.7462e-03,  5.5588e-02,  1.0463e-01],\n",
       "                        [-2.6578e-02,  5.8805e-03,  1.0929e-01,  3.4066e-02,  2.8816e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.6804e-03, -8.4820e-02, -5.8888e-02, -1.2364e-02,  4.0089e-02],\n",
       "                        [ 3.1759e-02,  5.9869e-02, -2.7704e-02,  1.8447e-02,  1.0067e-01],\n",
       "                        [-1.7151e-02, -9.1603e-02,  5.6622e-03, -9.6578e-02,  6.8389e-03],\n",
       "                        [ 6.7714e-02, -3.4104e-03,  7.0485e-02,  8.1749e-02, -8.8146e-02],\n",
       "                        [ 9.4248e-02, -7.5093e-02, -1.8915e-02, -4.2181e-02, -6.7813e-02]],\n",
       "              \n",
       "                       [[-5.6521e-02,  5.8057e-02, -4.1456e-02,  1.0363e-01,  4.0853e-02],\n",
       "                        [ 6.6468e-02,  1.1486e-02, -2.9433e-02, -8.9063e-02, -2.3259e-02],\n",
       "                        [-5.8600e-02,  4.6956e-02,  9.0166e-02, -5.6997e-02,  1.3046e-02],\n",
       "                        [ 8.2142e-02,  5.2639e-02,  2.2794e-02, -1.0978e-01, -9.8010e-04],\n",
       "                        [-4.7620e-02,  6.5324e-02, -1.4877e-02, -2.0812e-02,  7.2725e-02]],\n",
       "              \n",
       "                       [[ 1.0613e-01, -4.5007e-02, -1.9566e-02, -8.1833e-02, -5.7638e-02],\n",
       "                        [ 4.5985e-02,  1.0554e-01,  6.6870e-02, -9.2261e-04, -7.9116e-03],\n",
       "                        [ 6.0763e-02, -1.8339e-02,  5.1065e-02,  8.4758e-02, -1.1262e-01],\n",
       "                        [-6.4356e-02,  6.1005e-02, -1.1328e-01, -7.9766e-02,  3.5246e-02],\n",
       "                        [ 8.9707e-02,  1.6706e-02,  6.4345e-02,  2.7174e-03,  4.3414e-03]]]],\n",
       "                     dtype=torch.float64)),\n",
       "             ('conv_layers.0.bias',\n",
       "              tensor([ 0.0942, -0.0798,  0.0329], dtype=torch.float64)),\n",
       "             ('conv_layers.2.weight',\n",
       "              tensor([[[[-0.1072,  0.1517,  0.1232],\n",
       "                        [ 0.0166, -0.0279, -0.0699],\n",
       "                        [ 0.1262,  0.0333, -0.1595]],\n",
       "              \n",
       "                       [[-0.0831, -0.0033,  0.0786],\n",
       "                        [-0.0129, -0.1438,  0.0725],\n",
       "                        [ 0.1685, -0.1192,  0.1333]],\n",
       "              \n",
       "                       [[ 0.1376,  0.0153,  0.1668],\n",
       "                        [ 0.1775, -0.1150, -0.0875],\n",
       "                        [-0.1422, -0.1047,  0.0478]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0341, -0.0458,  0.1402],\n",
       "                        [ 0.1427, -0.0235,  0.0013],\n",
       "                        [-0.1626,  0.0613,  0.1655]],\n",
       "              \n",
       "                       [[ 0.0819,  0.1145,  0.0091],\n",
       "                        [ 0.1441, -0.0256, -0.0599],\n",
       "                        [-0.0162,  0.1915,  0.1382]],\n",
       "              \n",
       "                       [[-0.0324,  0.1891,  0.1488],\n",
       "                        [ 0.1707,  0.0634, -0.0620],\n",
       "                        [-0.0342,  0.0574,  0.1241]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1501,  0.0608,  0.0710],\n",
       "                        [-0.0382, -0.0580,  0.1175],\n",
       "                        [-0.0534,  0.0609,  0.0572]],\n",
       "              \n",
       "                       [[-0.0082,  0.1607,  0.0834],\n",
       "                        [ 0.0655, -0.1122, -0.1368],\n",
       "                        [-0.0768, -0.1142,  0.0870]],\n",
       "              \n",
       "                       [[ 0.1573, -0.1874,  0.0596],\n",
       "                        [ 0.1418, -0.1890,  0.1009],\n",
       "                        [-0.0571, -0.0005,  0.1397]]]], dtype=torch.float64)),\n",
       "             ('conv_layers.2.bias',\n",
       "              tensor([ 9.9513e-02, -1.2446e-01, -3.9162e-05], dtype=torch.float64)),\n",
       "             ('lin_layers.0.weight',\n",
       "              tensor([[-0.0029, -0.0080, -0.0047,  ...,  0.0189,  0.0165,  0.0215],\n",
       "                      [-0.0201,  0.0022,  0.0118,  ...,  0.0009, -0.0131,  0.0097],\n",
       "                      [ 0.0237,  0.0030,  0.0052,  ..., -0.0005,  0.0131, -0.0177],\n",
       "                      ...,\n",
       "                      [ 0.0057, -0.0192, -0.0013,  ..., -0.0241, -0.0150,  0.0186],\n",
       "                      [ 0.0179,  0.0072,  0.0082,  ...,  0.0031, -0.0210, -0.0229],\n",
       "                      [-0.0013,  0.0200, -0.0185,  ..., -0.0187, -0.0084, -0.0137]],\n",
       "                     dtype=torch.float64)),\n",
       "             ('lin_layers.0.bias',\n",
       "              tensor([-0.0093, -0.0194, -0.0007, -0.0009, -0.0147,  0.0244, -0.0257, -0.0189,\n",
       "                      -0.0113, -0.0034,  0.0139, -0.0188, -0.0143,  0.0164, -0.0005, -0.0250,\n",
       "                      -0.0193, -0.0002, -0.0123,  0.0079, -0.0128,  0.0040, -0.0237, -0.0024,\n",
       "                      -0.0155, -0.0242,  0.0207, -0.0076,  0.0018, -0.0174, -0.0007, -0.0110,\n",
       "                      -0.0080, -0.0006, -0.0095, -0.0092,  0.0197,  0.0262, -0.0086,  0.0012,\n",
       "                       0.0213, -0.0034,  0.0247, -0.0064,  0.0170, -0.0114, -0.0220, -0.0082,\n",
       "                       0.0225,  0.0004,  0.0127, -0.0097, -0.0148,  0.0103,  0.0174,  0.0023,\n",
       "                      -0.0097,  0.0074, -0.0215, -0.0098,  0.0051, -0.0037, -0.0067,  0.0059,\n",
       "                      -0.0234,  0.0085,  0.0097,  0.0055, -0.0124,  0.0200, -0.0132, -0.0159,\n",
       "                       0.0029, -0.0034,  0.0101, -0.0217,  0.0147,  0.0154,  0.0111, -0.0208,\n",
       "                       0.0157, -0.0007,  0.0226, -0.0034,  0.0183, -0.0082,  0.0249,  0.0087,\n",
       "                       0.0215,  0.0197,  0.0086, -0.0116, -0.0012, -0.0043, -0.0170,  0.0234,\n",
       "                      -0.0238,  0.0005,  0.0112,  0.0216], dtype=torch.float64)),\n",
       "             ('lin_layers.2.weight',\n",
       "              tensor([[-0.0349, -0.0689, -0.0245, -0.0327, -0.0193, -0.0261, -0.0590,  0.0719,\n",
       "                       -0.0018, -0.0131,  0.0164,  0.0296, -0.0806,  0.0844, -0.0156, -0.0045,\n",
       "                        0.0813, -0.0295,  0.0995, -0.0448, -0.0710, -0.0431,  0.0828, -0.0342,\n",
       "                       -0.0693, -0.0248, -0.0979, -0.0783,  0.0460, -0.0443, -0.0441, -0.0919,\n",
       "                       -0.0572,  0.0195, -0.0917, -0.0332,  0.0848,  0.0950, -0.0548, -0.0901,\n",
       "                        0.0392,  0.0113, -0.0171,  0.0971, -0.0399,  0.0639, -0.0565, -0.0107,\n",
       "                        0.0036, -0.0261, -0.0936,  0.0321,  0.0797,  0.0736,  0.0518, -0.0555,\n",
       "                       -0.0160,  0.0870, -0.0931, -0.0428, -0.0192, -0.0455, -0.0637,  0.0294,\n",
       "                        0.0244, -0.0232, -0.0936, -0.0954, -0.0662,  0.0988, -0.0115, -0.0429,\n",
       "                        0.0832,  0.0177,  0.0513, -0.0026,  0.0274,  0.0370,  0.0200,  0.0677,\n",
       "                        0.0432,  0.0592, -0.0890,  0.0781,  0.0581,  0.0476,  0.0528,  0.0694,\n",
       "                        0.0219, -0.0463,  0.0897,  0.0782, -0.0180,  0.0272,  0.0903, -0.0423,\n",
       "                       -0.0018, -0.0037, -0.0461, -0.0648],\n",
       "                      [ 0.0904, -0.0866,  0.0501,  0.0737, -0.0966, -0.0972,  0.0607,  0.0782,\n",
       "                        0.0121, -0.0497,  0.0937,  0.0124,  0.0913,  0.0839, -0.0409,  0.0927,\n",
       "                       -0.0772, -0.0615,  0.0184,  0.0442,  0.0141, -0.0506,  0.0541,  0.0680,\n",
       "                       -0.0877, -0.0594,  0.0538, -0.0939,  0.0974,  0.0906, -0.0460,  0.0604,\n",
       "                       -0.0927, -0.0606, -0.0601, -0.0307, -0.0227,  0.0864,  0.0039, -0.0873,\n",
       "                        0.0767, -0.0868, -0.0270, -0.0352, -0.0513, -0.0412,  0.0695,  0.0146,\n",
       "                       -0.0839,  0.0425, -0.0712,  0.0085, -0.0484,  0.0827, -0.0196,  0.0425,\n",
       "                        0.0092, -0.0244,  0.0959, -0.0579,  0.0049,  0.0491,  0.0032,  0.0959,\n",
       "                        0.0806,  0.0449, -0.0352,  0.0644, -0.0196,  0.0100,  0.0939,  0.0906,\n",
       "                       -0.0749, -0.0245,  0.0789,  0.0463, -0.0096,  0.0746, -0.0854,  0.0874,\n",
       "                       -0.0467,  0.0572,  0.0538, -0.0621, -0.0650, -0.0125,  0.0856,  0.0868,\n",
       "                       -0.0976,  0.0642,  0.0639, -0.0617,  0.0722,  0.0093,  0.0239, -0.0584,\n",
       "                       -0.0658, -0.0585,  0.0076,  0.0843]], dtype=torch.float64)),\n",
       "             ('lin_layers.2.bias',\n",
       "              tensor([-0.0225,  0.0924], dtype=torch.float64))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0cb5634b-ea75-43b1-88e5-e7426b433a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv_layers.0.weight',\n",
       "              tensor([[[[ 3.8847e-01,  3.4794e-01,  3.8714e-01,  3.3205e-01,  3.9644e-01],\n",
       "                        [ 4.4389e-01,  3.2961e-01,  2.6505e-01,  3.7597e-01,  3.9037e-01],\n",
       "                        [ 4.5140e-01,  3.0831e-01,  4.0820e-01,  3.5078e-01,  4.1184e-01],\n",
       "                        [ 3.0261e-01,  3.3138e-01,  3.4447e-01,  4.4523e-01,  3.6033e-01],\n",
       "                        [ 4.1520e-01,  4.1542e-01,  3.1002e-01,  3.1236e-01,  4.5467e-01]],\n",
       "              \n",
       "                       [[ 7.7870e-02,  2.3507e-01,  7.0241e-02,  2.5560e-01,  1.1234e-01],\n",
       "                        [ 2.4495e-02,  6.2042e-02,  1.9420e-01,  7.8308e-02,  2.3599e-01],\n",
       "                        [ 1.7683e-01,  1.7936e-01,  3.3385e-02,  1.7533e-01,  7.8519e-02],\n",
       "                        [ 8.2897e-02,  1.0974e-01,  1.4022e-01,  2.5273e-02,  6.6264e-02],\n",
       "                        [ 2.0980e-01,  1.8888e-01,  1.7943e-01,  1.5335e-01,  1.8464e-01]],\n",
       "              \n",
       "                       [[ 3.3025e-01,  1.6765e-01,  3.6639e-01,  3.0843e-01,  2.0634e-01],\n",
       "                        [ 3.5549e-01,  3.1492e-01,  3.0350e-01,  1.7519e-01,  2.6742e-01],\n",
       "                        [ 2.6865e-01,  1.3837e-01,  2.1555e-01,  2.1321e-01,  2.8908e-01],\n",
       "                        [ 1.7840e-01,  3.1539e-01,  1.9923e-01,  3.5535e-01,  3.2123e-01],\n",
       "                        [ 2.9245e-01,  2.3047e-01,  3.2493e-01,  3.4597e-01,  3.4596e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.6309e+00, -7.7367e+00, -7.7534e+00, -7.5971e+00, -7.7502e+00],\n",
       "                        [-7.5515e+00, -7.6533e+00, -7.6194e+00, -7.5363e+00, -7.6913e+00],\n",
       "                        [-7.6663e+00, -7.6519e+00, -7.5062e+00, -7.5314e+00, -7.6952e+00],\n",
       "                        [-7.6935e+00, -7.7075e+00, -7.5073e+00, -7.6969e+00, -7.5409e+00],\n",
       "                        [-7.6288e+00, -7.7526e+00, -7.5795e+00, -7.5898e+00, -7.5715e+00]],\n",
       "              \n",
       "                       [[-7.7221e+00, -7.6973e+00, -7.7374e+00, -7.6514e+00, -7.6810e+00],\n",
       "                        [-7.5655e+00, -7.4587e+00, -7.5526e+00, -7.4725e+00, -7.6330e+00],\n",
       "                        [-7.5755e+00, -7.4395e+00, -7.4880e+00, -7.5194e+00, -7.6453e+00],\n",
       "                        [-7.6518e+00, -7.4908e+00, -7.4942e+00, -7.5425e+00, -7.6785e+00],\n",
       "                        [-7.5914e+00, -7.6093e+00, -7.5792e+00, -7.6243e+00, -7.6225e+00]],\n",
       "              \n",
       "                       [[-7.3568e+00, -7.4086e+00, -7.2993e+00, -7.4427e+00, -7.3682e+00],\n",
       "                        [-7.3364e+00, -7.3863e+00, -7.2628e+00, -7.2355e+00, -7.3849e+00],\n",
       "                        [-7.2572e+00, -7.3284e+00, -7.3575e+00, -7.2839e+00, -7.2814e+00],\n",
       "                        [-7.2622e+00, -7.2584e+00, -7.3080e+00, -7.2564e+00, -7.2449e+00],\n",
       "                        [-7.4205e+00, -7.3593e+00, -7.2460e+00, -7.3295e+00, -7.3593e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.2391e-01,  2.4624e-01,  2.9226e-01,  3.2477e-01,  3.7897e-01],\n",
       "                        [ 3.3044e-01,  3.3790e-01,  2.7240e-01,  3.2542e-01,  4.2766e-01],\n",
       "                        [ 2.8507e-01,  1.8104e-01,  2.9228e-01,  2.1366e-01,  3.4914e-01],\n",
       "                        [ 3.6089e-01,  2.6753e-01,  3.6133e-01,  3.9378e-01,  2.5029e-01],\n",
       "                        [ 4.0808e-01,  2.3518e-01,  3.1073e-01,  2.9499e-01,  2.7874e-01]],\n",
       "              \n",
       "                       [[ 7.2164e-02,  1.8382e-01,  1.1155e-01,  2.3662e-01,  1.7629e-01],\n",
       "                        [ 1.4941e-01,  5.7134e-02,  4.6489e-02, -2.4781e-03,  9.4225e-02],\n",
       "                        [ 3.2493e-02,  8.6166e-02,  1.4712e-01,  3.5762e-02,  1.5445e-01],\n",
       "                        [ 1.6026e-01,  9.1380e-02,  9.0362e-02, -8.6381e-03,  1.3774e-01],\n",
       "                        [ 6.0678e-02,  1.6482e-01,  1.1369e-01,  1.2094e-01,  2.2543e-01]],\n",
       "              \n",
       "                       [[ 3.5576e-01,  2.0455e-01,  2.4652e-01,  1.7384e-01,  1.9753e-01],\n",
       "                        [ 2.7101e-01,  3.0962e-01,  2.8880e-01,  2.2831e-01,  2.3678e-01],\n",
       "                        [ 2.9217e-01,  1.8441e-01,  2.6395e-01,  3.1859e-01,  1.4803e-01],\n",
       "                        [ 1.5963e-01,  2.6332e-01,  1.0665e-01,  1.6010e-01,  2.9584e-01],\n",
       "                        [ 3.3103e-01,  2.5451e-01,  3.1978e-01,  2.6769e-01,  2.7454e-01]]]],\n",
       "                     dtype=torch.float64)),\n",
       "             ('conv_layers.0.bias',\n",
       "              tensor([ 0.4906, -7.4792,  0.3915], dtype=torch.float64)),\n",
       "             ('conv_layers.2.weight',\n",
       "              tensor([[[[ 4.2370,  4.5690,  4.4585],\n",
       "                        [ 4.4362,  4.4901,  4.3416],\n",
       "                        [ 4.4696,  4.4510,  4.1779]],\n",
       "              \n",
       "                       [[ 8.3190,  8.3343,  8.4835],\n",
       "                        [ 8.3215,  8.1101,  8.4148],\n",
       "                        [ 8.5741,  8.2272,  8.5481]],\n",
       "              \n",
       "                       [[ 4.5493,  4.4951,  4.5636],\n",
       "                        [ 4.6631,  4.4636,  4.3838],\n",
       "                        [ 4.2687,  4.3775,  4.4494]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.4906, -4.4610, -4.3240],\n",
       "                        [-4.2666, -4.3650, -4.4162],\n",
       "                        [-4.6178, -4.3360, -4.2771]],\n",
       "              \n",
       "                       [[-6.7006, -6.7070, -6.7822],\n",
       "                        [-6.6725, -6.8920, -6.8804],\n",
       "                        [-6.8024, -6.6214, -6.6353]],\n",
       "              \n",
       "                       [[-4.6076, -4.3512, -4.4408],\n",
       "                        [-4.3595, -4.4059, -4.6075],\n",
       "                        [-4.6082, -4.4625, -4.4409]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.6852, -0.3605, -0.4595],\n",
       "                        [-0.4544, -0.3251, -0.2953],\n",
       "                        [-0.5738, -0.3452, -0.4648]],\n",
       "              \n",
       "                       [[ 6.4755,  6.6456,  6.5678],\n",
       "                        [ 6.5397,  6.3540,  6.3294],\n",
       "                        [ 6.3932,  6.3481,  6.5503]],\n",
       "              \n",
       "                       [[-0.3141, -0.5501, -0.4134],\n",
       "                        [-0.2124, -0.3998, -0.2565],\n",
       "                        [-0.5140, -0.3463, -0.3222]]]], dtype=torch.float64)),\n",
       "             ('conv_layers.2.bias',\n",
       "              tensor([-0.8599, -2.1256,  2.3756], dtype=torch.float64)),\n",
       "             ('lin_layers.0.weight',\n",
       "              tensor([[10.0616, 10.0195,  9.9338,  ..., -3.5605, -3.5044, -3.5061],\n",
       "                      [ 6.3022,  6.4318,  6.3585,  ..., -3.0961, -3.2028, -3.2161],\n",
       "                      [ 8.5848,  8.5424,  8.6070,  ..., -1.8317, -1.8603, -1.9034],\n",
       "                      ...,\n",
       "                      [ 0.5430,  0.5125,  0.5187,  ...,  0.4289,  0.4034,  6.3564],\n",
       "                      [10.8146, 10.7287, 10.6521,  ..., -3.0124, -2.9604, -2.9841],\n",
       "                      [ 8.4174,  8.4561,  8.4199,  ..., -1.6846, -1.7222, -1.7724]],\n",
       "                     dtype=torch.float64)),\n",
       "             ('lin_layers.0.bias',\n",
       "              tensor([-23.1699,  -5.7711, -14.1052, -11.5087, -14.1474, -15.7457, -15.1220,\n",
       "                      -15.3514, -14.5965, -14.1058, -15.0835, -14.1066, -22.7078, -14.0587,\n",
       "                       -7.5038, -15.1696,  -7.5367,  -8.9808, -14.1176, -15.1540, -11.5136,\n",
       "                      -14.0759, -14.1203, -15.1380, -14.1029,  -7.5287, -15.0094, -14.0938,\n",
       "                      -13.8706, -15.0866, -14.0761, -22.6942, -14.1095, -14.7534, -15.2902,\n",
       "                      -15.3516, -14.1356,  -5.6931, -15.2320, -15.3393, -11.4669, -14.1525,\n",
       "                      -14.0580,  -7.0153, -14.0660, -14.1655, -14.7779, -22.0253,  -8.9378,\n",
       "                      -15.2033, -15.2868, -14.1019, -14.1853, -11.5558, -14.1111, -14.6665,\n",
       "                      -15.3039,  -9.4442, -14.9702, -14.0951, -15.2917, -15.1517, -15.2135,\n",
       "                      -22.7287, -15.2533, -21.8249, -14.1094, -15.0154, -15.0995,  -5.0938,\n",
       "                      -23.0580, -15.0838,  -9.4008, -14.1093, -12.0391, -15.2649,  -5.4373,\n",
       "                      -13.9033, -14.1437, -15.3268, -14.1254, -14.0375, -23.1302, -14.1846,\n",
       "                       -7.4952, -14.1278, -15.2516, -15.3011,  -5.0456, -15.0964,  -5.5524,\n",
       "                       -7.5269, -13.7833, -14.0918, -14.1409, -14.0634,  -7.1432,  -7.5060,\n",
       "                      -22.6036, -14.9878], dtype=torch.float64)),\n",
       "             ('lin_layers.2.weight',\n",
       "              tensor([[ 6.4312,  3.4691,  2.3606,  1.7504,  3.2364,  1.7289,  1.3529,  1.4654,\n",
       "                        2.1317,  3.2680,  1.4682,  3.3224,  5.2865,  3.3876,  0.8797,  1.4028,\n",
       "                        0.9754,  3.6059,  3.3701,  1.3622,  1.6831,  3.2559,  3.3688,  1.3739,\n",
       "                        3.2232,  0.8712,  1.3191,  3.2157,  2.2992,  1.3702,  3.2583,  6.6750,\n",
       "                        3.2246,  2.0375,  1.3049,  1.3584,  3.3212,  3.5472,  1.3463,  1.3032,\n",
       "                        1.7202,  3.2537,  3.2800,  0.3271,  3.2566,  3.3017,  1.5358,  4.5295,\n",
       "                        3.4544,  1.3770,  1.3029,  3.3213,  3.3025,  1.7955,  3.3114,  1.8756,\n",
       "                        1.3798,  2.6943,  1.3297,  3.2516,  1.3764,  1.3631,  1.3399,  5.6542,\n",
       "                        1.4252,  4.0299,  2.2810,  1.3232,  1.4295,  2.7654,  6.2488,  1.3705,\n",
       "                        2.4208,  3.2955,  1.4571,  1.3973,  3.1530,  2.3005,  3.2574,  1.4624,\n",
       "                        3.2911,  3.3894,  6.5976,  3.2925,  0.9526,  3.3144,  1.4500,  1.4645,\n",
       "                        2.5841,  1.3650,  3.3334,  0.9727,  2.2256,  3.3199,  3.3531,  3.2513,\n",
       "                        1.0737,  0.8910,  5.3124,  1.3703],\n",
       "                      [-6.3756, -3.6246, -2.3350, -1.7094, -3.3524, -1.8522, -1.3512, -1.3152,\n",
       "                       -2.1214, -3.3308, -1.3580, -3.2804, -5.2758, -3.2192, -0.9362, -1.3147,\n",
       "                       -0.9712, -3.6970, -3.2522, -1.3628, -1.7400, -3.3495, -3.2318, -1.3400,\n",
       "                       -3.3802, -0.9554, -1.3632, -3.3879, -2.1557, -1.3239, -3.3483, -6.7065,\n",
       "                       -3.3745, -2.0786, -1.4568, -1.4223, -3.2591, -3.3658, -1.3971, -1.4806,\n",
       "                       -1.6043, -3.3291, -3.3242, -0.2652, -3.3478, -3.2789, -1.5227, -4.5255,\n",
       "                       -3.5347, -1.3606, -1.4676, -3.2806, -3.2712, -1.6392, -3.2791, -1.8886,\n",
       "                       -1.3867, -2.6318, -1.3270, -3.3523, -1.3907, -1.3595, -1.4004, -5.5289,\n",
       "                       -1.3202, -4.0082, -2.4098, -1.3542, -1.5153, -2.6567, -6.1664, -1.3227,\n",
       "                       -2.4125, -3.3024, -1.3269, -1.3536, -3.1352, -2.1889, -3.3228, -1.3073,\n",
       "                       -3.2945, -3.2731, -6.6328, -3.2765, -0.9595, -3.2792, -1.3116, -1.3083,\n",
       "                       -2.6598, -1.3471, -3.1798, -0.9562, -2.1713, -3.2834, -3.2389, -3.3520,\n",
       "                       -1.1413, -0.9532, -5.3509, -1.3507]], dtype=torch.float64)),\n",
       "             ('lin_layers.2.bias',\n",
       "              tensor([ 2.4265, -2.3566], dtype=torch.float64))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69cdff1d-7250-4f0a-a539-e5786743550b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4885, 0.4885, 0.4885, 0.4885, 0.4885, 0.4885, 0.4885, 0.4885,\n",
       "       0.4885, 0.4885])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62f5c654-fd67-4ea2-b7e5-e8467c6fdcef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8d22ec1f70>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(epochs), accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
